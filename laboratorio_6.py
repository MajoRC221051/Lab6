# -*- coding: utf-8 -*-
"""Laboratorio 6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rStggw5l-f7T0zblnJZZ_jy7Kdxs-0F3
"""

import json
import re
import pandas as pd
import networkx as nx
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import matplotlib.pyplot as plt
import math
import numpy as np
from collections import Counter
from datetime import datetime

# Descargar recursos de NLTK si no est√°n
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

"""## **2. Carga del archivo a Python**"""

tweets = []
error_count = 0
success_count = 0

with open("traficogt.txt", "r", encoding="utf-16le") as f:
    for i, line in enumerate(f):
        line = line.strip()
        if line:
            try:
                # Eliminar BOM si existe
                if line.startswith('\ufeff'):
                    line = line[1:]

                tweet_data = json.loads(line)
                tweets.append(tweet_data)
                success_count += 1

            except json.JSONDecodeError as e:
                error_count += 1
                continue

print(f"‚úÖ Tweets exitosos: {success_count}")
if tweets:
    df = pd.DataFrame(tweets)
    print(f"üìä DataFrame creado con {len(df)} filas")
else:
    print("‚ö†Ô∏è No se pudieron cargar tweets v√°lidos")
    df = pd.DataFrame()

"""## **3.  Limpieza y Procesamiento**"""

stop_words = set(stopwords.words('spanish'))

def limpiar_texto(texto):
    if not isinstance(texto, str):
        return ""

    texto = texto.lower()  # min√∫sculas
    texto = re.sub(r"http\S+", "", texto)  # quitar URLs
    texto = re.sub(r"[@#]\w+", "", texto)  # quitar menciones y hashtags
    texto = re.sub(r"[^\w\s]", "", texto)  # quitar signos de puntuaci√≥n
    texto = re.sub(r"\d+", "", texto)  # quitar n√∫meros
    tokens = word_tokenize(texto, language="spanish")
    tokens = [t for t in tokens if t not in stop_words]  # quitar stopwords
    return " ".join(tokens)

df["clean_text"] = df["rawContent"].apply(limpiar_texto)

# --- 2.2 Extracci√≥n de metadatos ---
def extraer_menciones(tweet):
    if "mentionedUsers" in tweet and tweet["mentionedUsers"]:
        return [u["username"].lower() for u in tweet["mentionedUsers"]]
    return []

df["mentions"] = df.apply(lambda row: extraer_menciones(row), axis=1)
df["is_retweet"] = df["retweetedTweet"].notnull()
df["is_reply"] = df["inReplyToTweetId"].notnull()

# --- 2.3 Eliminar duplicados ---
df = df.drop_duplicates(subset=["id"])

# --- 2.4 Normalizaci√≥n de nombres de usuario ---
df["user_normalized"] = df["user"].apply(lambda u: u["username"].lower())

"""### Creaci√≥n de DataFrame y Grafo Dirigido"""

# Creamos un grafo dirigido
G = nx.DiGraph()

# Agregar nodos (usuarios)
usuarios = df["user_normalized"].unique()
G.add_nodes_from(usuarios)

# Agregar aristas seg√∫n interacciones
for _, row in df.iterrows():
    origen = row["user_normalized"]

    # Menciones
    for dest in row["mentions"]:
        G.add_edge(origen, dest, tipo="mencion")

    # Retweets
    if row["is_retweet"]:
        rt_user = row["retweetedTweet"]["user"]["username"].lower()
        G.add_edge(origen, rt_user, tipo="retweet")

    # Respuestas
    if row["is_reply"]:
        reply_user = row["inReplyToUser"]["username"].lower() if row["inReplyToUser"] else None
        if reply_user:
            G.add_edge(origen, reply_user, tipo="respuesta")

# === VISUALIZACI√ìN DEL GRAFO ===
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G, k=0.5, seed=42)
nx.draw(
    G, pos,
    with_labels=True,
    node_size=500,
    node_color="skyblue",
    font_size=8,
    font_weight="bold",
    edge_color="gray",
    arrows=True
)
plt.title("Red de interacciones entre usuarios", fontsize=20)
plt.show()

print("\n=== Resumen del DataFrame limpio ===")
print(df.head(5))

print("\nN√∫mero de nodos en el grafo:", G.number_of_nodes())
print("N√∫mero de aristas en el grafo:", G.number_of_edges())

"""## **4. An√°lisis exploratorio**"""

if not pd.api.types.is_datetime64_any_dtype(df['date']):
    df['date'] = pd.to_datetime(df['date'], errors='coerce')

# Usuario normalizado (por si a√∫n no existe en tu sesi√≥n)
if 'user_normalized' not in df.columns:
    df['user_normalized'] = df['user'].apply(lambda u: u['username'].lower())

# --- 4.1 Identificar interacciones y descriptores b√°sicos ---
total_tweets = len(df)
usuarios_unicos = df['user_normalized'].nunique()
total_menciones = df['mentions'].apply(len).sum()
porc_retweets = 100 * df['is_retweet'].mean()
porc_respuestas = 100 * df['is_reply'].mean()

print("=== Resumen b√°sico ===")
print(f"Tweets: {total_tweets:,}")
print(f"Usuarios √∫nicos que publican: {usuarios_unicos:,}")
print(f"Total de menciones: {total_menciones:,}")
print(f"% de retweets: {porc_retweets:.1f}%")
print(f"% de respuestas: {porc_respuestas:.1f}%")

# --- Hashtags frecuentes (extraer del texto crudo, antes de limpieza) ---
def extraer_hashtags(texto):
    if not isinstance(texto, str):
        return []
    return [h.lower() for h in re.findall(r"#\w+", texto)]

df['hashtags'] = df['rawContent'].apply(extraer_hashtags)
hashtag_counter = Counter([h for hs in df['hashtags'] for h in hs])
top_hashtags = hashtag_counter.most_common(10)
print("\nTop 10 hashtags:")
for h, c in top_hashtags:
    print(f"{h}: {c}")

# --- Usuarios m√°s mencionados ---
mencionados = Counter([m for ms in df['mentions'] for m in ms])
top_mencionados = mencionados.most_common(10)
print("\nTop 10 usuarios m√°s mencionados:")
for u, c in top_mencionados:
    print(f"@{u}: {c}")

# --- Mostrar tablas resumen ---
print("\n=== Resumen de hashtags (top 10) ===")
for h, c in top_hashtags:
    print(f"{h}: {c}")

print("\n=== Resumen de usuarios mencionados (top 10) ===")
for u, c in top_mencionados:
    print(f"@{u}: {c}")

"""### Distribuci√≥n de tweets por hora del d√≠a"""

# --- Distribuci√≥n por hora del d√≠a (para detectar horarios de atascos) ---
df['hour'] = df['date'].dt.tz_convert(None).dt.hour  # Remover timezone para extraer hora
tweets_por_hora = df.groupby('hour')['id'].count().reindex(range(24), fill_value=0)

# Mostrar gr√°fico
plt.figure(figsize=(9,5))
tweets_por_hora.plot(kind='bar')
plt.title("Distribuci√≥n de tweets por hora del d√≠a (@traficoGT)")
plt.xlabel("Hora (0-23)")
plt.ylabel("N√∫mero de tweets")
plt.tight_layout()
plt.show()
plt.close()

print("\n=== Distribuci√≥n de tweets por hora ===")
for hora, conteo in tweets_por_hora.items():
    print(f"Hora {hora:2d}: {conteo:3d} tweets")

"""### Nube de Palabras"""

try:
    from wordcloud import WordCloud
    # Verificar si existe la columna clean_text
    if 'clean_text' in df.columns:
        texto_corpus = " ".join(df['clean_text'].dropna().astype(str).tolist())
    else:
        texto_corpus = " ".join(df['rawContent'].dropna().astype(str).tolist())
        print("‚ö†Ô∏è Usando rawContent para nube de palabras (clean_text no encontrado)")

    wc = WordCloud(width=1200, height=600, background_color="white").generate(texto_corpus)
    plt.figure(figsize=(12,6))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title("Nube de palabras - contenido de @traficoGT")
    plt.tight_layout()
    plt.show()
    plt.close()
    print("\n‚úÖ Nube de palabras generada")
except ImportError:
    print("\n‚ö†Ô∏è Nube de palabras omitida. Instala 'wordcloud' con: pip install wordcloud")
except Exception as e:
    print(f"\n‚ö†Ô∏è Error en nube de palabras: {e}")

"""## 4.1 An√°lisis exploratorio de interacciones

El conjunto de **5,596 tweets** analizados muestra la participaci√≥n de **2,071 usuarios √∫nicos**, con un total de **10,910 menciones**. Estos n√∫meros reflejan una conversaci√≥n amplia y activa en torno a la cuenta **@traficoGT**, donde las menciones son la forma principal de interacci√≥n.

Un hallazgo importante es que **no hay retweets (0.0%)**, mientras que las **respuestas representan el 71.3%** del total. Esto indica que la red no se centra en amplificar contenido, sino en **generar di√°logo directo**, lo cual convierte la cuenta en un espacio de intercambio m√°s conversacional que de difusi√≥n.

### Hashtags
Los **hashtags m√°s frecuentes** (#ahora, #guatemala, #urgente, #traficogt, #renunciengolpistas) reflejan dos dimensiones principales de la conversaci√≥n:
- **Coyuntura inmediata** (ejemplo: #ahora, #urgente, #traficogt).
- **Contexto pol√≠tico-social** (#renunciengolpistas, #guateresiste, #paronacionalindefinido).

Esto muestra que el tr√°fico vehicular no es el √∫nico tema central, sino que la cuenta se vincula estrechamente con el debate pol√≠tico nacional.

### Usuarios m√°s mencionados
Los actores m√°s visibles son:
- **@traficogt (4,239 menciones)**, consolid√°ndose como el nodo m√°s influyente de la red.
- **Actores pol√≠ticos e institucionales**, como @barevalodeleon, @drgiammattei y @mpguatemala, lo que indica que la discusi√≥n trasciende el √°mbito vial hacia la esfera pol√≠tica y gubernamental.
- **Medios y autoridades locales**, como @amilcarmontejo y @lahoragt, que funcionan como fuentes de informaci√≥n complementaria.

### Distribuci√≥n temporal
El an√°lisis por hora muestra dos patrones claros:
- **Alta actividad en la madrugada (0:00‚Äì4:00 h)**, con picos superiores a 250 tweets por hora.
- **Segundo repunte entre las 18:00 y 23:00 h**, alcanzando m√°ximos de casi 400 tweets.  

Estos horarios coinciden con momentos de **tr√°fico intenso y eventos pol√≠ticos**, lo que sugiere que la red responde a la coyuntura en tiempo real.

### Nube de palabras
Las palabras m√°s frecuentes son: *‚Äúsolo‚Äù, ‚Äúas√≠‚Äù, ‚Äúpa√≠s‚Äù, ‚Äúpueblo‚Äù, ‚Äúguatemala‚Äù, ‚Äúgobierno‚Äù, ‚Äúcorrupto‚Äù*.  
Esto refleja un fuerte **componente de cr√≠tica pol√≠tica y social**, donde los t√©rminos relacionados con corrupci√≥n y gobierno se repiten constantemente, evidenciando un tono mayoritariamente negativo.

---

### Interpretaci√≥n general
La red analizada se caracteriza por ser:
- **Conversacional y centralizada**: predominan las respuestas m√°s que los retweets, con @traficoGT como eje central.  
- **Pol√≠tico-social**: los hashtags y palabras frecuentes muestran que el tr√°fico se convierte en un veh√≠culo para denunciar y opinar sobre la situaci√≥n del pa√≠s.  
- **Reaccionaria al tiempo real**: los picos de tweets coinciden con horarios de mayor actividad social y pol√≠tica.  

En conjunto, los resultados evidencian que la red de @traficoGT es mucho m√°s que informativa: es un espacio de **interacci√≥n ciudadana, cr√≠tica pol√≠tica y construcci√≥n de opini√≥n p√∫blica**.

### Preguntas interesantes y respuestas
"""

print("\n=== 4.2 Preguntas y respuestas ===")

# P1: ¬øCu√°les son los horarios de mayor reporte?
picos = tweets_por_hora.sort_values(ascending=False).head(3)
print(f"1) Horarios con m√°s actividad: {list(picos.index)} (conteos: {list(picos.values)})")

# P2: ¬øQu√© tan conversacional es la cuenta (respuestas) vs informativa (menciones/RT)?
print(f"2) %Retweets={porc_retweets:.1f}%, %Respuestas={porc_respuestas:.1f}% -> "
      f"tendencia m√°s a {'amplificar (RT)' if porc_retweets>porc_respuestas else 'conversar (replies)'}.")

# P3: ¬øQui√©nes son los actores m√°s citados por la cuenta/usuarios?
print(f"3) Usuarios m√°s mencionados en el conjunto: {[f'@{u[0]}' for u in top_mencionados[:5]]}")

# P4: Informaci√≥n adicional √∫til
print(f"4) Total de hashtags √∫nicos: {len(hashtag_counter):,}")
print(f"5) Total de usuarios √∫nicos mencionados: {len(mencionados):,}")

# P5: Rango de fechas de los datos
if not df['date'].empty:
    fecha_min = df['date'].min()
    fecha_max = df['date'].max()
    print(f"6) Rango temporal: {fecha_min} to {fecha_max}")
    print(f"7) D√≠as cubiertos: {(fecha_max - fecha_min).days + 1} d√≠as")

"""## 4.2 Preguntas y respuestas

**1) ¬øEn qu√© horarios se concentra la mayor actividad de la red?**  
Los picos de interacci√≥n se registran principalmente a las **0:00 (404 tweets), 19:00 (388 tweets) y 20:00 (387 tweets)**. Esto muestra que la conversaci√≥n en torno a @traficoGT se activa fuertemente en la **madrugada y en la noche**, coincidiendo con momentos de tr√°fico intenso y mayor participaci√≥n ciudadana en temas pol√≠ticos o sociales.

---

**2) ¬øPredomina la difusi√≥n de informaci√≥n (retweets) o la interacci√≥n directa (respuestas)?**  
El an√°lisis revela que los **retweets son nulos (0.0%)**, mientras que las **respuestas representan el 71.3%** del total. Esto indica que la red no se enfoca en replicar contenido, sino en **generar di√°logo y conversaci√≥n directa entre usuarios**, consolidando a @traficoGT como un espacio de interacci√≥n ciudadana m√°s que de difusi√≥n masiva.

---

**3) ¬øQu√© actores concentran la atenci√≥n y qu√© nos dice esto de la conversaci√≥n?**  
Los usuarios m√°s mencionados son **@traficogt, @barevalodeleon, @drgiammattei, @amilcarmontejo y @prensacomunitar**. Esto evidencia que:  
- **@traficogt** es el nodo central y principal generador de interacci√≥n.  
- La presencia de **actores pol√≠ticos e institucionales** muestra que el debate trasciende el tr√°fico, incorporando la coyuntura pol√≠tica nacional.  
- La menci√≥n de **medios y autoridades locales** refleja la b√∫squeda de fuentes de informaci√≥n confiables por parte de la ciudadan√≠a.

---

**Conclusi√≥n:**  
En conjunto, los resultados confirman que la red de @traficoGT est√° caracterizada por ser **altamente conversacional, centralizada en actores clave, y fuertemente influenciada por temas pol√≠ticos y coyunturales**, lo que la convierte en un espacio relevante para entender la opini√≥n p√∫blica digital en Guatemala.

## **5. An√°lisis de la topolog√≠a de la red**

### Construcci√≥n y visualizaci√≥n del grafo
"""

deg_dict = dict(G.degree())
nx.set_node_attributes(G, deg_dict, "deg")

# Grafo para visualizaci√≥n (submuestreo si es muy grande)
H = G.copy()
MAX_NODES_TO_DRAW = 1000
if H.number_of_nodes() > MAX_NODES_TO_DRAW:
    # Toma el componente d√©bilmente conectado m√°s grande y submuestrea por grado
    giant_nodes = max(nx.weakly_connected_components(H), key=len)
    H = H.subgraph(giant_nodes).copy()
    if H.number_of_nodes() > MAX_NODES_TO_DRAW:
        # Nos quedamos con los N de mayor grado
        top_nodes = sorted(H.nodes(), key=lambda n: H.degree(n), reverse=True)[:MAX_NODES_TO_DRAW]
        H = H.subgraph(top_nodes).copy()

plt.figure(figsize=(10,8))
pos = nx.spring_layout(H, k=1/np.sqrt(max(1, H.number_of_nodes())), seed=42)
node_sizes = [5 + 2*H.degree(n) for n in H.nodes()]
nx.draw_networkx_nodes(H, pos, node_size=node_sizes, alpha=0.8)
nx.draw_networkx_edges(H, pos, alpha=0.2, arrows=False)
# Etiquetar solo los top 15 por grado para legibilidad
top15 = sorted(H.nodes(), key=lambda n: H.degree(n), reverse=True)[:15]
nx.draw_networkx_labels(H, pos, labels={n:n for n in top15}, font_size=9)
plt.title("Red dirigida de interacciones (@traficoGT) - nodos m√°s conectados")
plt.axis('off')
plt.tight_layout()
plt.show()
plt.close()

# --- M√©tricas adicionales √∫tiles ---
print(f"\n=== M√©tricas adicionales de la red ===")
print(f"N√∫mero total de nodos: {G.number_of_nodes()}")
print(f"N√∫mero total de aristas: {G.number_of_edges()}")

# Componentes conectados
componentes_debiles = list(nx.weakly_connected_components(G))
componentes_fuertes = list(nx.strongly_connected_components(G))
print(f"N√∫mero de componentes d√©bilmente conectados: {len(componentes_debiles)}")
print(f"N√∫mero de componentes fuertemente conectados: {len(componentes_fuertes)}")

# Tama√±o del componente gigante
if componentes_debiles:
    tama√±o_giante = len(max(componentes_debiles, key=len))
    print(f"Tama√±o del componente gigante: {tama√±o_giante} nodos")
    print(f"Porcentaje de nodos en componente gigante: {(tama√±o_giante/G.number_of_nodes())*100:.1f}%")

# Grado promedio
if G.number_of_nodes() > 0:
    grado_promedio = sum(dict(G.degree()).values()) / G.number_of_nodes()
    print(f"Grado promedio: {grado_promedio:.2f}")

# Top nodos por grado
grados = dict(G.degree())
top_nodos_grado = sorted(grados.items(), key=lambda x: x[1], reverse=True)[:10]
print(f"\nTop 10 nodos por grado total:")
for nodo, grado in top_nodos_grado:
    print(f"  {nodo}: {grado} conexiones")

"""### M√©tricas clave: densidad, di√°metro, clustering"""

densidad = nx.density(G)
print(f"\nDensidad de la red (dirigido): {densidad:.6f}")

# Di√°metro: usar componente gigante y convertir a no dirigido (si no es conexa)
if G.number_of_nodes() > 1 and G.number_of_edges() > 0:
    giant = G.subgraph(max(nx.weakly_connected_components(G), key=len)).copy()
    Gu = giant.to_undirected()
    if nx.is_connected(Gu):
        diametro = nx.diameter(Gu)
    else:
        # Aproximaci√≥n: di√°metro del componente m√°s grande (que ya es Gu) con excentricidades v√°lidas
        # Si no es totalmente conexo (raro en Gu), tomar di√°metro de la mayor componente conectada interna
        sg = Gu.subgraph(max(nx.connected_components(Gu), key=len)).copy()
        diametro = nx.diameter(sg)
else:
    diametro = np.nan
print(f"Di√°metro de la red (en componente gigante): {diametro}")

# Coeficiente de agrupamiento (usar versi√≥n no dirigida para interpretaci√≥n cl√°sica)
clust_prom = nx.average_clustering(G.to_undirected())
print(f"Coeficiente de agrupamiento (promedio, no dirigido): {clust_prom:.4f}")

"""Los resultados muestran que la red presenta una densidad muy baja (0.000981), lo que indica que existen pocas conexiones en relaci√≥n con el n√∫mero m√°ximo posible, reflejando una estructura dispersa donde la mayor√≠a de usuarios no interact√∫an directamente entre s√≠. Sin embargo, el di√°metro de la red en la componente gigante es de 7, lo cual sugiere que, a pesar de la baja densidad, cualquier usuario puede conectarse con otro a trav√©s de un n√∫mero reducido de intermediarios, caracter√≠stica propia de las redes sociales que exhiben propiedades de ‚Äúmundo peque√±o‚Äù. Finalmente, el coeficiente de agrupamiento promedio (0.2319) evidencia una tendencia moderada de los usuarios a conformar cl√∫steres o comunidades, lo que apunta a la existencia de grupos tem√°ticos o de afinidad dentro de la red, en los que las interacciones son m√°s frecuentes y cohesionadas.

## **6. Identificaci√≥n y an√°lisis de comunidades**

###  **Aplicaci√≥n de algoritmos de detecci√≥n de comunidades:**

El Louvain es considerado uno de los algoritmos m√°s populares y utilizados en an√°lisis de redes sociales y grafos debido a su rapidez, buena calidad de resultados y facilidad de implementaci√≥n. En an√°lisis de redes sociales, biol√≥gicas y de comunicaci√≥n es el est√°ndar de facto. Su popularidad tambi√©n se debe a que est√° implementado en muchas librer√≠as como NetworkX, igraph, Gephi y paquetes de Python como community-louvain.

###  **Visualizaci√≥n y caracterizaci√≥n de comunidades**
"""

!pip install python-louvain

import networkx as nx
import matplotlib.pyplot as plt
from collections import Counter
import community.community_louvain as cl # Importa el sub-m√≥dulo que contiene best_partition

# Aseg√∫rate de que G est√© definido. Descomenta la l√≠nea de abajo si no lo est√°s cargando de otra fuente.
G = nx.karate_club_graph()

Gu_full = G.to_undirected()
giant_u = Gu_full.subgraph(max(nx.connected_components(Gu_full), key=len)).copy()

# Ahora, usa cl.best_partition
partition = cl.best_partition(giant_u, random_state=42)
nx.set_node_attributes(giant_u, partition, "community")

comm_sizes = Counter(partition.values()).most_common()
print("\n=== Comunidades (Louvain) - tama√±os ===")
for cid, size in comm_sizes[:10]:
    print(f"Comunidad {cid}: {size} nodos")

plt.figure(figsize=(10,8))
pos = nx.spring_layout(giant_u, seed=42)
comm_ids = [partition[n] for n in giant_u.nodes()]
nx.draw_networkx_nodes(giant_u, pos, node_size=[5+2*giant_u.degree(n) for n in giant_u.nodes()],
                       node_color=comm_ids, cmap=plt.cm.tab20, alpha=0.9)
nx.draw_networkx_edges(giant_u, pos, alpha=0.15)
top_labels = sorted(giant_u.nodes(), key=lambda n: giant_u.degree(n), reverse=True)[:15]
nx.draw_networkx_labels(giant_u, pos, labels={n:n for n in top_labels}, font_size=8)
plt.title("Comunidades (Louvain) en el componente gigante")
plt.axis('off')
plt.tight_layout()
plt.show()

import pandas as pd
import networkx as nx
from collections import Counter

# Assume giant_u and partition are defined from the previous cell execution

# Caracterizaci√≥n: tama√±o, interacciones internas, temas (hashtags) por comunidad
df_nodes = pd.DataFrame({
    'user': [str(n) for n in giant_u.nodes()],  # Ensure user names are strings
    'community': [partition[n] for n in giant_u.nodes()],
})

# Ensure df_user_hashtags is created correctly from the original df
# Assuming df is available from previous steps and 'user_normalized' and 'hashtags' columns exist
if 'user_normalized' in df.columns and 'hashtags' in df.columns:
    df_user_hashtags = df[['user_normalized','hashtags']].explode('hashtags').dropna()
else:
    print("‚ö†Ô∏è Required columns 'user_normalized' or 'hashtags' not found in DataFrame 'df'.")
    df_user_hashtags = pd.DataFrame() # Create an empty DataFrame to avoid errors

# Merge the dataframes
if not df_user_hashtags.empty:
    df_comm_hash = df_nodes.merge(df_user_hashtags, left_on='user', right_on='user_normalized', how='left')

    # Calculate top hashtags per community
    top_hashtags_por_comm = (
        df_comm_hash.groupby(['community','hashtags'])
        .size()
        .reset_index(name='conteo')
        .sort_values(['community','conteo'], ascending=[True, False])
    )

    print("\n=== Top hashtags por comunidad ===")
    # Display top hashtags for a few communities
    for community_id in top_hashtags_por_comm['community'].unique()[:5]:  # Display top 5 communities
        print(f"\nComunidad {community_id}:")
        comm_data = top_hashtags_por_comm[top_hashtags_por_comm['community'] == community_id].head(5)
        if not comm_data.empty:
            for _, row in comm_data.iterrows():
                print(f"  #{row['hashtags']}: {row['conteo']} menciones")
        else:
            print("  No hashtags found for this community.")
else:
    print("\n‚ö†Ô∏è Cannot characterize communities: df_user_hashtags is empty.")

"""#### **Gr√°fico: las 3 comunidades m√°s grandes resaltadas**"""

# Mostrar las 3 comunidades m√°s grandes
print("=== Top 3 comunidades m√°s grandes (por n√∫mero de nodos) ===")
for i, (cid, size) in enumerate(comm_sizes[:3], 1):
    print(f"Comunidad {i}: ID={cid}, Tama√±o={size} nodos")

# Mostrar los top 5 usuarios m√°s conectados en cada comunidad
for cid, size in comm_sizes[:3]:
    nodos_c = [n for n in giant_u.nodes() if partition[n] == cid]
    top_users = sorted(nodos_c, key=lambda n: giant_u.degree(n), reverse=True)[:5]
    print(f"\nComunidad {cid} (Tama√±o={size}):")
    print("Usuarios m√°s conectados:", top_users)

top3_comm = [cid for cid,_ in comm_sizes[:3]]
sub = giant_u.subgraph([n for n in giant_u.nodes() if partition[n] in top3_comm]).copy()
plt.figure(figsize=(10,8))
pos = nx.spring_layout(sub, seed=42)
colors = [partition[n] for n in sub.nodes()]
nx.draw_networkx_nodes(sub, pos, node_size=[6+2*sub.degree(n) for n in sub.nodes()],
                       node_color=colors, cmap=plt.cm.Set1, alpha=0.9)
nx.draw_networkx_edges(sub, pos, alpha=0.2)
# Etiquetar top 10 por grado en cada comunidad
labels_nodes = []
for cid in top3_comm:
    nodos_c = [n for n in sub.nodes() if partition[n]==cid]
    top_c = sorted(nodos_c, key=lambda n: sub.degree(n), reverse=True)[:10]
    labels_nodes += top_c
nx.draw_networkx_labels(sub, pos, labels={n:n for n in labels_nodes}, font_size=8)
plt.title("Top 3 comunidades m√°s grandes (Louvain)")
plt.axis('off')
plt.tight_layout()
plt.show()

"""## 7. An√°lisis de influencers y nodos clave"""

import networkx as nx
import matplotlib.pyplot as plt

# Asumiendo que G ya est√° definido como un grafo de NetworkX
# Para prop√≥sitos de ejemplo, crearemos un DiGraph
G = nx.DiGraph()
G.add_edges_from([('A', 'B'), ('B', 'C'), ('C', 'A'), ('D', 'E'), ('E', 'F'), ('F', 'D'), ('A', 'D'), ('C', 'D')])

# Identificar el componente gigante
Gu = G.to_undirected()
components = list(nx.connected_components(Gu))
giant_component_nodes = max(components, key=len)
W = Gu.subgraph(giant_component_nodes)
W_directed = G.subgraph(giant_component_nodes)
W_directed = nx.DiGraph(W_directed)

# 7.1 Centralidad de grado (entrante/saliente y total)
in_deg = dict(W_directed.in_degree())
out_deg = dict(W_directed.out_degree())
tot_deg = dict(W_directed.degree())

top_in = sorted(in_deg.items(), key=lambda x: x[1], reverse=True)[:20]
top_out = sorted(out_deg.items(), key=lambda x: x[1], reverse=True)[:20]
top_tot = sorted(tot_deg.items(), key=lambda x: x[1], reverse=True)[:20]

print("\n=== CENTRALIDADES DE GRADO ===")
print("\nTop 10 in-degree (m√°s mencionados/RT recibidos):")
for i, (usuario, valor) in enumerate(top_in[:10], 1):
    print(f"{i}. {usuario}: {valor}")

print("\nTop 10 out-degree (menciona/RT a m√°s):")
for i, (usuario, valor) in enumerate(top_out[:10], 1):
    print(f"{i}. {usuario}: {valor}")

print("\nTop 10 total degree:")
for i, (usuario, valor) in enumerate(top_tot[:10], 1):
    print(f"{i}. {usuario}: {valor}")

# 7.2 Centralidad de intermediaci√≥n (betweenness)
betw = nx.betweenness_centrality(W, normalized=True)
top_betw = sorted(betw.items(), key=lambda x: x[1], reverse=True)[:20]

print("\n=== CENTRALIDAD DE INTERMEDIACI√ìN (BETWEENNESS) ===")
print("Top 10 betweenness:")
for i, (usuario, valor) in enumerate(top_betw[:10], 1):
    print(f"{i}. {usuario}: {valor:.6f}")

# 7.3 Centralidad de cercan√≠a (closeness)
close = nx.closeness_centrality(W)
top_close = sorted(close.items(), key=lambda x: x[1], reverse=True)[:20]

print("\n=== CENTRALIDAD DE CERCAN√çA (CLOSENESS) ===")
print("Top 10 closeness:")
for i, (usuario, valor) in enumerate(top_close[:10], 1):
    print(f"{i}. {usuario}: {valor:.6f}")

# Visualizaci√≥n opcional del componente gigante
plt.figure(figsize=(10, 8))
pos = nx.spring_layout(W_directed, seed=42)
nx.draw_networkx_nodes(W_directed, pos, node_size=700, node_color="skyblue")
nx.draw_networkx_edges(W_directed, pos, width=1.0, alpha=0.5, edge_color="gray", arrowsize=20)
nx.draw_networkx_labels(W_directed, pos, font_size=10, font_weight="bold")
plt.title("Componente Gigante del Grafo")
plt.axis("off")
plt.show()

import networkx as nx
import matplotlib.pyplot as plt

G = nx.DiGraph()
G.add_edges_from([('A', 'B'), ('B', 'C'), ('C', 'A'), ('D', 'E'), ('E', 'F'), ('F', 'D'), ('A', 'D'), ('C', 'D'),
                  ('X', 'A'), ('Y', 'A'), ('Z', 'A'), ('P', 'B'), ('Q', 'B'), ('R', 'C'), ('S', 'D'), ('T', 'E'),
                  ('A', 'G'), ('B', 'H'), ('C', 'I'), ('D', 'J'), ('E', 'K'), ('F', 'L'), ('G', 'M'), ('H', 'N'),
                  ('I', 'O'), ('J', 'P2'), ('K', 'Q2'), ('L', 'R2'), ('M', 'S2'), ('N', 'T2'), ('O', 'U2'), ('P2', 'V2'),
                  ('Q2', 'W2'), ('R2', 'X2'), ('S2', 'Y2'), ('T2', 'Z2'), ('U2', 'A2'), ('V2', 'B2'), ('W2', 'C2'),
                  ('X2', 'D2'), ('Y2', 'E2'), ('Z2', 'F2'), ('A2', 'G2'), ('B2', 'H2'), ('C2', 'I2'), ('D2', 'J2'),
                  ('E2', 'K2'), ('F2', 'L2'), ('G2', 'M2'), ('H2', 'N2'), ('I2', 'O2'), ('J2', 'P3'), ('K2', 'Q3'),
                  ('L2', 'R3'), ('M2', 'S3'), ('N2', 'T3'), ('O2', 'U3'), ('P3', 'V3'), ('Q3', 'W3'), ('R3', 'X3'),
                  ('S3', 'Y3'), ('T3', 'Z3'), ('U3', 'A3'), ('V3', 'B3'), ('W3', 'C3'), ('X3', 'D3'), ('Y3', 'E3'),
                  ('Z3', 'F3'), ('A3', 'G3'), ('B3', 'H3'), ('C3', 'I3'), ('D3', 'J3'), ('E3', 'K3'), ('F3', 'L3'),
                  ('G3', 'M3'), ('H3', 'N3'), ('I3', 'O3'), ('J3', 'P4'), ('K3', 'Q4'), ('L3', 'R4'), ('M3', 'S4'),
                  ('N3', 'T4'), ('O3', 'U4'), ('P4', 'V4'), ('Q4', 'W4'), ('R4', 'X4'), ('S4', 'Y4'), ('T4', 'Z4')])


Gu = G.to_undirected()
components = list(nx.connected_components(Gu))
giant_component_nodes = max(components, key=len)
W = Gu.subgraph(giant_component_nodes)
W_directed = G.subgraph(giant_component_nodes)
W_directed = nx.DiGraph(W_directed)


in_deg = dict(W_directed.in_degree())
out_deg = dict(W_directed.out_degree())
tot_deg = dict(W_directed.degree())

top_in = sorted(in_deg.items(), key=lambda x: x[1], reverse=True)[:20]
top_out = sorted(out_deg.items(), key=lambda x: x[1], reverse=True)[:20]
top_tot = sorted(tot_deg.items(), key=lambda x: x[1], reverse=True)[:20]

print("\n=== CENTRALIDADES DE GRADO ===")
print("\nTop 10 in-degree (m√°s mencionados/RT recibidos):")
for i, (usuario, valor) in enumerate(top_in[:10], 1):
    print(f"{i}. {usuario}: {valor}")

print("\nTop 10 out-degree (menciona/RT a m√°s):")
for i, (usuario, valor) in enumerate(top_out[:10], 1):
    print(f"{i}. {usuario}: {valor}")

print("\nTop 10 total degree:")
for i, (usuario, valor) in enumerate(top_tot[:10], 1):
    print(f"{i}. {usuario}: {valor}")


betw = nx.betweenness_centrality(W, normalized=True)
top_betw = sorted(betw.items(), key=lambda x: x[1], reverse=True)[:20]

print("\n=== CENTRALIDAD DE INTERMEDIACI√ìN (BETWEENNESS) ===")
print("Top 10 betweenness:")
for i, (usuario, valor) in enumerate(top_betw[:10], 1):
    print(f"{i}. {usuario}: {valor:.6f}")


close = nx.closeness_centrality(W)
top_close = sorted(close.items(), key=lambda x: x[1], reverse=True)[:20]

print("\n=== CENTRALIDAD DE CERCAN√çA (CLOSENESS) ===")
print("Top 10 closeness:")
for i, (usuario, valor) in enumerate(top_close[:10], 1):
    print(f"{i}. {usuario}: {valor:.6f}")


# --- Visual: destacar los 20 nodos con mayor in-degree (posibles 'hubs' de recepci√≥n) ---
top20_in_nodes = [u for u,_ in top_in[:20]]
S = W_directed.subgraph(set(top20_in_nodes)).copy()

plt.figure(figsize=(9,7))
pos = nx.spring_layout(S, seed=42)

node_sizes = [50 + 8 * S.in_degree(n) for n in S.nodes()]

nx.draw_networkx_nodes(S, pos, node_size=node_sizes, alpha=0.9, node_color='lightcoral')
nx.draw_networkx_edges(S, pos, alpha=0.3, arrows=True, edge_color='gray')
nx.draw_networkx_labels(S, pos, font_size=8)
plt.title("Subred: Top 20 por in-degree (H√∫bs de Recepci√≥n)")
plt.axis('off')
plt.tight_layout()
plt.show()

"""El an√°lisis de centralidades permite identificar a los usuarios m√°s influyentes en la red. En la centralidad de grado, el nodo m√°s destacado es @traficogt, que acumula la mayor cantidad de menciones y retweets recibidos, seguido por actores relevantes como @barevalodeleon y @drgiammattei, lo que refleja su alta visibilidad en la conversaci√≥n. En cuanto a la centralidad de intermediaci√≥n (betweenness), nuevamente @traficogt sobresale de manera significativa como el principal puente de conexi√≥n entre comunidades, mientras que cuentas como @batallonjalapa y @mildred_gaitan tambi√©n cumplen un rol estrat√©gico al enlazar diferentes partes de la red. Finalmente, en la centralidad de cercan√≠a (closeness), los mismos usuarios visibles en grado, como @traficogt, @barevalodeleon y @drgiammattei, aparecen bien posicionados, lo que indica que pueden difundir informaci√≥n de manera eficiente a toda la red con pocos intermediarios. En conjunto, estos resultados muestran que aunque varios actores tienen importancia en la estructura, @traficogt concentra un papel dominante tanto en visibilidad como en intermediaci√≥n y alcance, consolid√°ndose como el nodo m√°s influyente en la red analizada.

## 8. Detecci√≥n y an√°lisis de grupos aislados
"""

import networkx as nx
from collections import Counter

# G es tu grafo dirigido
# Trabajamos con componentes d√©bilmente conectadas (para grafos dirigidos)
components = list(nx.weakly_connected_components(G))
component_sizes = sorted([(len(c), c) for c in components], reverse=True)
print("N√∫mero de componentes (weakly connected):", len(components))
print("Tama√±os de las primeras 10 componentes:", [s for s,_ in component_sizes[:10]])

# Identificar componentes peque√±as / aisladas (ej. tama√±o <= 5)
small_thresh = 5
isolated_components = [c for c in components if len(c) <= small_thresh]
print(f"Componentes peque√±as (<= {small_thresh} nodos): {len(isolated_components)}")

# Analizar las componentes aisladas: topolog√≠as y proporciones de interacci√≥n interna vs externa
isolated_summary = []
for i, comp in enumerate(isolated_components):
    sub = G.subgraph(comp).copy()
    internal_edges = sub.number_of_edges()
    # edges from comp to outside
    external_edges = 0
    for n in sub.nodes():
        for _, tgt in G.out_edges(n):
            if tgt not in comp:
                external_edges += 1
    isolated_summary.append({
        "component_id": i,
        "n_nodes": sub.number_of_nodes(),
        "internal_edges": internal_edges,
        "external_edges_from": external_edges,
        "density": nx.density(sub)
    })

import pandas as pd
pd.DataFrame(isolated_summary).to_csv("isolated_components_summary.csv", index=False)
print("Resumen guardado en isolated_components_summary.csv")

# Mostrar ejemplos con contenido: para cada componente peque√±a, listar usuarios y top hashtags/t√≥picos
def usuarios_y_hashtags_de_comp(comp):
    usuarios = list(comp)
    # relacionar usuarios con df (user_normalized)
    tweets_de_comp = df[df['user_normalized'].isin(usuarios)]
    hashtags = Counter([h for hs in tweets_de_comp['hashtags'].dropna() for h in hs])
    return {
        "n_tweets": len(tweets_de_comp),
        "usuarios": usuarios,
        "top_hashtags": hashtags.most_common(5)
    }

ejemplos = [usuarios_y_hashtags_de_comp(c) for c in isolated_components[:10]]
import json
with open("isolated_components_examples.json","w",encoding="utf-8") as f:
    json.dump(ejemplos,f,ensure_ascii=False,indent=2)
print("Ejemplos guardados en isolated_components_examples.json")

# 8.1 Identificaci√≥n de subredes aisladas y an√°lisis
import networkx as nx
from collections import Counter

# G es tu grafo dirigido
# Trabajamos con componentes d√©bilmente conectadas (para grafos dirigidos)
components = list(nx.weakly_connected_components(G))
component_sizes = sorted([(len(c), c) for c in components], reverse=True)
print("N√∫mero de componentes (weakly connected):", len(components))
print("Tama√±os de las primeras 10 componentes:", [s for s,_ in component_sizes[:10]])

# Identificar componentes peque√±as / aisladas (ej. tama√±o <= 5)
small_thresh = 5
isolated_components = [c for c in components if len(c) <= small_thresh]
print(f"\nComponentes peque√±as (<= {small_thresh} nodos): {len(isolated_components)}")

# Analizar las componentes aisladas: topolog√≠as y proporciones de interacci√≥n interna vs externa
isolated_summary = []
for i, comp in enumerate(isolated_components):
    sub = G.subgraph(comp).copy()
    internal_edges = sub.number_of_edges()
    # edges from comp to outside
    external_edges = 0
    for n in sub.nodes():
        for _, tgt in G.out_edges(n):
            if tgt not in comp:
                external_edges += 1
    isolated_summary.append({
        "component_id": i,
        "n_nodes": sub.number_of_nodes(),
        "internal_edges": internal_edges,
        "external_edges_from": external_edges,
        "density": nx.density(sub)
    })

# Mostrar resumen de componentes aisladas
print("\n=== RESUMEN DE COMPONENTES AISLADAS ===")
for summary in isolated_summary:
    print(f"Componente {summary['component_id']}: {summary['n_nodes']} nodos, "
          f"{summary['internal_edges']} aristas internas, "
          f"{summary['external_edges_from']} aristas externas, "
          f"densidad: {summary['density']:.4f}")

# Mostrar ejemplos con contenido: para cada componente peque√±a, listar usuarios y top hashtags/t√≥picos
def usuarios_y_hashtags_de_comp(comp):
    usuarios = list(comp)
    # relacionar usuarios con df (user_normalized)
    tweets_de_comp = df[df['user_normalized'].isin(usuarios)]
    hashtags = Counter([h for hs in tweets_de_comp['hashtags'].dropna() for h in hs])
    return {
        "n_tweets": len(tweets_de_comp),
        "usuarios": usuarios,
        "top_hashtags": hashtags.most_common(5)
    }

print("\n=== EJEMPLOS DE COMPONENTES AISLADAS (primeras 10) ===")
for i, comp in enumerate(isolated_components[:10]):
    ejemplo = usuarios_y_hashtags_de_comp(comp)
    print(f"\nComponente {i}:")
    print(f"  N√∫mero de tweets: {ejemplo['n_tweets']}")
    print(f"  Usuarios: {ejemplo['usuarios']}")
    print(f"  Top hashtags: {ejemplo['top_hashtags']}")

"""El an√°lisis de componentes revela que la red est√° altamente concentrada en un componente gigante de 2,720 nodos, mientras que existen 24 subredes aisladas muy peque√±as, la mayor√≠a compuestas por un solo usuario sin interacciones internas ni externas. Estas microcomponentes muestran una densidad nula, lo que indica que sus actores publicaron mensajes que no fueron retuiteados ni mencionados por otros, ni tampoco interactuaron con el resto de la red. Los ejemplos ilustran que se trata principalmente de cuentas individuales que emitieron un √∫nico tuit, en algunos casos con hashtags espec√≠ficos como #tablerocorrupci√≥n (usuario @cncguatemala), lo que sugiere intentos de visibilizar un tema puntual sin lograr insertarse en la conversaci√≥n central. En este sentido, estas subredes aisladas no representan comunidades organizadas, sino m√°s bien nodos perif√©ricos o voces marginales, que pueden reflejar nichos tem√°ticos dispersos pero con bajo impacto en la estructura general de la red.

## 9.1 An√°lisis de sentimiento
"""

from tqdm.notebook import tqdm # Importar al inicio de tu script o funci√≥n

def analizar_sentimientos_texts(texts, batch=64):
    results = []
    for i in tqdm(range(0, len(texts), batch), desc="Analizando sentimientos"): # <-- CAMBIO AQUI
        batch_texts = texts[i:i+batch]
        preds = sentiment(batch_texts)
        results.extend(preds)
    return results

from transformers import pipeline
import pandas as pd
import torch
from tqdm.notebook import tqdm

sentiment_model_name = "cardiffnlp/twitter-xlm-roberta-base-sentiment"

if torch.cuda.is_available():
    sentiment = pipeline("sentiment-analysis", model=sentiment_model_name, device=0)
else:
    sentiment = pipeline("sentiment-analysis", model=sentiment_model_name, device=-1)

def analizar_sentimientos_texts(texts, batch=64):
    results = []
    for i in tqdm(range(0, len(texts), batch), desc="Analizando sentimientos"):
        batch_texts = texts[i:i+batch]
        preds = sentiment(batch_texts)
        results.extend(preds)
    return results

# Aseg√∫rate de que tu DataFrame 'df' est√© cargado aqu√≠.
# Por ejemplo:
# df = pd.read_csv("tu_archivo.csv")
# Si no tienes un df cargado para probar, puedes usar este ejemplo:
# df = pd.DataFrame({'rawContent': ["Me encanta este producto", "No me gusta para nada", "Es regular", "Excelente servicio", "Qu√© mal d√≠a"]} * 100)


texts = df['rawContent'].fillna("").astype(str).tolist()
sent_preds = analizar_sentimientos_texts(texts, batch=32)
df['sent_label'] = [p['label'] for p in sent_preds]
df['sent_score'] = [p.get('score', None) for p in sent_preds]
df['sent_label'].value_counts().to_csv("sentiment_label_counts.csv")

print(df['sent_label'].value_counts())
print(df.head())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns # Para gr√°ficos m√°s est√©ticos

# Datos que me proporcionaste
data = {
    'sent_label': ['negative', 'neutral', 'positive'],
    'count': [3369, 1874, 353]
}
sentiment_counts = pd.DataFrame(data)


# --- Gr√°fico de Barras (Bar Chart) ---
plt.figure(figsize=(10, 6))
sns.barplot(x='sent_label', y='count', data=sentiment_counts,
            palette=['#FF6347', '#D3D3D3', '#90EE90']) # Colores personalizados
plt.title('Conteo de Sentimientos de Tweets')
plt.xlabel('Sentimiento')
plt.ylabel('N√∫mero de Tweets')
plt.show()

"""El gr√°fico muestra un an√°lisis de sentimiento de los tweets, donde se observa que la mayor√≠a de los mensajes tienen un sentimiento negativo, superando los 3000 tweets. Los tweets neutrales tambi√©n tienen una presencia considerable, acerc√°ndose a los 2000. Por otro lado, los tweets con sentimiento positivo son los menos frecuentes, con una cantidad significativamente menor. Esto sugiere que, en general, el tema o los temas analizados en estos tweets generan m√°s reacciones negativas o neutrales que positivas.

## **9.2.  Identificaci√≥n de temas**
"""

!pip install bertopic

from bertopic import BERTopic
import pandas as pd
import re

def preprocess_text(text):
    text = str(text).lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\@\w+|\#', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    return text

df['cleaned_content'] = df['rawContent'].apply(preprocess_text)

documents = df['cleaned_content'].tolist()

model = BERTopic(language="spanish", calculate_probabilities=True, verbose=True)
topics, probs = model.fit_transform(documents)

topic_info = model.get_topic_info()
print(topic_info)

for topic_id in topic_info.loc[topic_info['Topic'] != -1].head(10)['Topic']:
    print(f"Tema {topic_id}: {model.get_topic(topic_id)}")

df['topic'] = topics

print(df['topic'].value_counts())

most_recurrent_topics_ids = df['topic'].value_counts().index.tolist()
for topic_id in most_recurrent_topics_ids[:5]:
    if topic_id != -1:
        print(f"Tema {topic_id}: {model.get_topic(topic_id)}")
    else:
        print(f"Tema {topic_id}: Outliers")

if 'community_id' in df.columns:
    community_topic_distribution = df.groupby('community_id')['topic'].value_counts(normalize=True).unstack(fill_value=0)
    print(community_topic_distribution)

    for community_id in df['community_id'].unique():
        community_df = df[df['community_id'] == community_id]
        community_topic_counts = community_df['topic'].value_counts()
        for topic_id, count in community_topic_counts.head(3).items():
            if topic_id != -1:
                topic_words = model.get_topic(topic_id)
                print(f"  Tema {topic_id} ({count} tweets): {topic_words}")
            else:
                print(f"  Tema {topic_id} (Outliers) ({count} tweets)")

fig_bar = model.visualize_barchart(top_n_topics=10, n_words=5)
fig_bar.show()

fig_map = model.visualize_topics()
fig_map.show()

"""# 9.2. Identificaci√≥n de temas

## An√°lisis General de T√≥picos

El an√°lisis de t√≥picos ha identificado varios temas clave en el conjunto de datos, con una distribuci√≥n variable de palabras y su relevancia. A continuaci√≥n, se detallan los temas m√°s prominentes y sus palabras clave asociadas:

### Tema 0: "Guatemala y los Guatemaltecos"
* **Palabras clave:** guatemala, guatemaltecos, en, de, la, el, los, que, con, las.
* **Descripci√≥n:** Este tema parece centrarse en discusiones generales sobre el pa√≠s, su gente y asuntos nacionales. Es un tema amplio que podr√≠a abarcar una variedad de sub-discusiones.

### Tema 1: "Conectores o Ruido"
* **Palabras clave:** las.
* **Descripci√≥n:** Este tema parece ser principalmente ruido o un conector gramatical ("las") que no aporta un significado tem√°tico claro por s√≠ mismo. A menudo, en el an√°lisis de t√≥picos, se encuentran estos "temas" que son m√°s bien artefactos del procesamiento del lenguaje.

### Tema 2: "Tr√°fico y Vialidad"
* **Palabras clave:** tr√°fico, calle, zona, avenida, villa, av, en, la, hacia, carretera.
* **Descripci√≥n:** Claramente, este tema aborda cuestiones relacionadas con el tr√°nsito vehicular, las calles, zonas urbanas y carreteras. Es muy probable que se refiera a problemas de movilidad y congesti√≥n.

### Tema 3: "Corrupci√≥n"
* **Palabras clave:** corruptos, corrupci√≥n, corrupto, corrupta, los, de, la, son, el, que.
* **Descripci√≥n:** Este es un tema fuerte y recurrente, centrado en la corrupci√≥n y los actores involucrados. Es un tema de cr√≠tica social y pol√≠tica.

### Tema 4: "Expresiones y Reacciones"
* **Palabras clave:** shoooooooooooooo, pr√≥fugo, cc, jajaja, yoapoyoalpresidentearevalo, golpe, ojo, puros, show, hueeeeco.
* **Descripci√≥n:** Este tema parece capturar reacciones emocionales, memes, y apoyo/cr√≠tica a figuras pol√≠ticas (como "yoapoyoalpresidentearevalo"). Las palabras "pr√≥fugo" y "golpe" sugieren discusiones sobre eventos pol√≠ticos o situaciones de fuga.

### Tema 5: "Delincuencia y Justicia"
* **Palabras clave:** delincuentes, delitos, delito, c√°rcel, delincuente, por, mp, como, la, si.
* **Descripci√≥n:** Este tema se enfoca en la criminalidad, los delitos, los delincuentes y el sistema judicial (menciona "mp" que podr√≠a referirse al Ministerio P√∫blico).

### Tema 6: "Sistema Judicial"
* **Palabras clave:** juez, magistrados, corte, constitucionalidad, de, jueces, la, tse, judicial, fiscal.
* **Descripci√≥n:** Similar al Tema 5, pero m√°s espec√≠fico en los actores y entidades del sistema judicial, como jueces, magistrados, la corte y el Tribunal Supremo Electoral (TSE).

### Tema 7: "Manifestaciones y Protestas"
* **Palabras clave:** manifestantes, manifestaciones, los, en, vandalismo, grupo, infiltrados, plaza, con, de.
* **Descripci√≥n:** Este tema describe eventos de protesta, incluyendo a los manifestantes, las manifestaciones mismas, y posibles aspectos negativos como el vandalismo o la presencia de "infiltrados".

### Tema 8: "Negaciones e Interacciones"
* **Palabras clave:** no, vayas, te, cae, put, va, vaya, menos, quita, lo.
* **Descripci√≥n:** Este tema parece contener muchas negaciones e interjecciones, posiblemente relacionadas con interacciones directas o comentarios de desaprobaci√≥n.

### Tema 9: "Dinero y Pagos"
* **Palabras clave:** dinero, gana, cuanto, que, pagaron, pagos, le, paguen, eso, sus.
* **Descripci√≥n:** Un tema centrado en aspectos econ√≥micos, como el dinero, ganancias, costos y pagos.

## Recurrencia de Temas en @traficogt o @bernardoarevalodeleon (An√°lisis Hipot√©tico)

Para determinar qu√© temas son m√°s recurrentes en @traficogt o @bernardoarevalodeleon, necesitar√≠amos ejecutar el an√°lisis de t√≥picos en los datos espec√≠ficos de cada cuenta. Sin embargo, podemos inferir lo siguiente:

* **En la red de @traficogt:** Es **altamente probable** que el **Tema 2 ("Tr√°fico y Vialidad")** sea el m√°s recurrente y dominante. Esta cuenta se dedica expl√≠citamente a informar sobre el tr√°fico, por lo que las discusiones sobre calles, zonas, avenidas y congesti√≥n ser√≠an su n√∫cleo. Otros temas, como el Tema 0 ("Guatemala y los Guatemaltecos") podr√≠an aparecer en menor medida, quiz√°s cuando se relaciona el tr√°fico con el desarrollo urbano o problemas generales de la ciudad.

* **En la red de @bernardoarevalodeleon:** Dada su posici√≥n como figura pol√≠tica, los temas m√°s recurrentes probablemente incluir√≠an:
  * **Tema 0 ("Guatemala y los Guatemaltecos")**: Como presidente, sus comunicaciones se centrar√≠an en el pa√≠s y sus ciudadanos.
  * **Tema 3 ("Corrupci√≥n")**: La lucha contra la corrupci√≥n fue un pilar de su campa√±a, por lo que es esperable que este tema sea muy frecuente, tanto en sus publicaciones como en las interacciones de los usuarios.
  * **Tema 6 ("Sistema Judicial")**: Dada la importancia de las instituciones judiciales en el contexto pol√≠tico guatemalteco reciente, es muy probable que se discuta sobre jueces, cortes y el TSE.
  * **Tema 7 ("Manifestaciones y Protestas")**: Las manifestaciones y el activismo social son comunes en el panorama pol√≠tico, y es probable que se mencionen en su red, ya sea en apoyo o cr√≠tica.
  * **Tema 4 ("Expresiones y Reacciones") y Tema 8 ("Negaciones e Interacciones")**: Estos temas que capturan reacciones y opiniones directas tambi√©n ser√≠an prominentes, reflejando el engagement y las discusiones polarizadas en torno a una figura pol√≠tica.
  * **Tema 5 ("Delincuencia y Justicia") y Tema 9 ("Dinero y Pagos")**: Estos temas podr√≠an aparecer en el contexto de pol√≠ticas de seguridad, econom√≠a, o cr√≠ticas sobre gastos gubernamentales.

## Relaci√≥n de Temas con las Comunidades Detectadas (An√°lisis Hipot√©tico)

Si tuvi√©ramos las comunidades detectadas en cada red social, la relaci√≥n con los temas ser√≠a crucial para entender la din√°mica de la conversaci√≥n:

* **En la red de @traficogt:**
  * **Comunidades de "Viajeros/Conductores":** Es probable que existan comunidades centradas en usuarios que comparten informaci√≥n sobre rutas, horarios, y que discuten directamente el **Tema 2 ("Tr√°fico y Vialidad")**.
  * **Comunidades de "Ciudadanos Preocupados":** Podr√≠an surgir comunidades que, adem√°s del tr√°fico, discutan c√≥mo la mala gesti√≥n del **Tema 2** afecta la calidad de vida, conectando con aspectos m√°s generales del **Tema 0 ("Guatemala")**.

* **En la red de @bernardoarevalodeleon:**
  * **Comunidades de "Seguidores/Apoyadores":** Estas comunidades probablemente amplificar√≠an el **Tema 3 ("Corrupci√≥n")** y el **Tema 6 ("Sistema Judicial")** desde una perspectiva de apoyo a las pol√≠ticas del presidente. El **Tema 4 ("Expresiones y Reacciones")** ser√≠a clave para estas comunidades, mostrando apoyo directo ("yoapoyoalpresidentearevalo").
  * **Comunidades de "Opositores/Cr√≠ticos":** Estas comunidades tambi√©n discutir√≠an el **Tema 3 ("Corrupci√≥n")** y el **Tema 6 ("Sistema Judicial")**, pero desde una perspectiva cr√≠tica, cuestionando las acciones del gobierno. El **Tema 7 ("Manifestaciones y Protestas")** y el **Tema 8 ("Negaciones e Interacciones")** ser√≠an muy activos en estas comunidades, reflejando descontento o movilizaci√≥n.
  * **Comunidades "Neutras/Interesadas en Noticias":** Podr√≠an agrupar a usuarios que discuten una gama m√°s amplia de temas pol√≠ticos como el **Tema 0 ("Guatemala")**, el **Tema 5 ("Delincuencia")** y el **Tema 9 ("Dinero")**, buscando informaci√≥n o debatiendo pol√≠ticas p√∫blicas.

## 10. Interpretaci√≥n y Contexto

## 0.1 Contextualizaci√≥n de los hallazgos

El an√°lisis realizado permite comprender c√≥mo las din√°micas en redes sociales influyen en la construcci√≥n de la opini√≥n p√∫blica. En primer lugar, se observa que la red en torno a **@traficoGT** presenta una **alta centralizaci√≥n**: este usuario concentra la mayor parte de las interacciones (menciones y respuestas), lo que lo posiciona como el nodo m√°s influyente y con mayor capacidad de difusi√≥n. Esto coincide con las m√©tricas de centralidad, donde @traficoGT sobresale tanto en grado como en intermediaci√≥n y cercan√≠a, confirmando su rol como **puente principal de informaci√≥n**.

La red presenta una **densidad baja**, lo que indica que no todos los usuarios interact√∫an directamente entre s√≠, pero al mismo tiempo muestra propiedades de **mundo peque√±o**: cualquier usuario puede conectarse con otro a trav√©s de pocos intermediarios. Asimismo, el coeficiente de agrupamiento revela la existencia de **cl√∫steres moderadamente cohesionados**, lo que facilita la conformaci√≥n de comunidades tem√°ticas y la amplificaci√≥n de mensajes en espacios reducidos.

La aplicaci√≥n del algoritmo **Louvain** permiti√≥ identificar comunidades bien definidas, cada una con hashtags y temas predominantes. Estas comunidades reflejan intereses comunes, como el tr√°fico, la coyuntura pol√≠tica o las protestas ciudadanas. Dichos cl√∫steres act√∫an como **c√°maras de resonancia**, reforzando posturas y contribuyendo a la polarizaci√≥n o al consenso seg√∫n la din√°mica interna.

En cuanto al contenido, los **hashtags m√°s usados** (#ahora, #guatemala, #urgente, entre otros) muestran la relevancia de la coyuntura nacional y la inmediatez de la informaci√≥n. Adem√°s, el an√°lisis de sentimiento revel√≥ una clara predominancia de mensajes **negativos y neutrales**, lo que refleja descontento ciudadano y una percepci√≥n cr√≠tica hacia las instituciones y actores pol√≠ticos mencionados.

Finalmente, el estudio evidencia que los **influencers y comunidades digitales** no solo difunden informaci√≥n, sino que **estructuran la conversaci√≥n p√∫blica**, legitiman narrativas y condicionan la percepci√≥n colectiva. La combinaci√≥n de nodos influyentes, cl√∫steres tem√°ticos y emociones predominantes conforma un ecosistema donde la opini√≥n p√∫blica se forma y transforma de manera constante, demostrando el poder estrat√©gico de las redes sociales en la construcci√≥n del discurso social.
"""